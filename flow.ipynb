{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3fdd99-60c2-4294-a08f-9cc7c7670ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, PodSpec\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext\n",
    ")\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "import torch\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bf5bc2-38fc-400f-8b0b-604f977effe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x570ea for key /Type\n",
      "Multiple definitions in dictionary at byte 0x570ea for key /Type\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x570f9 for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x570f9 for key /Subtype\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x5710d for key /BBox\n",
      "Multiple definitions in dictionary at byte 0x5710d for key /BBox\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x57159 for key /Resources\n",
      "Multiple definitions in dictionary at byte 0x57159 for key /Resources\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x59cd7 for key /Type\n",
      "Multiple definitions in dictionary at byte 0x59cd7 for key /Type\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x59ce6 for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x59ce6 for key /Subtype\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x59d09 for key /BBox\n",
      "Multiple definitions in dictionary at byte 0x59d09 for key /BBox\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x59d55 for key /Resources\n",
      "Multiple definitions in dictionary at byte 0x59d55 for key /Resources\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"./Data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8418f0ba-7370-444d-bdb5-687a9f4a6739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19857.91 ms\n",
      "llama_print_timings:      sample time =       0.97 ms /    10 runs   (    0.10 ms per token, 10256.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3790.32 ms /     5 tokens (  758.06 ms per token,     1.32 tokens per second)\n",
      "llama_print_timings:        eval time =    1297.76 ms /     9 runs   (  144.20 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:       total time =    5105.77 ms /    14 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=' Hello! How can I assist you today?', additional_kwargs={}, raw={'id': 'cmpl-8814763e-9271-4cc5-b579-900b31d78102', 'object': 'text_completion', 'created': 1712263372, 'model': '/Users/adityatodi/Library/Caches/llama_index/models/mistral-7b-instruct-v0.1.Q8_0.gguf', 'choices': [{'text': ' Hello! How can I assist you today?', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 9, 'total_tokens': 73}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894a38b0-4ffa-4493-b1f3-feafe117d387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/adityatodi/Library/Caches/llama_index/models/mistral-7b-instruct-v0.1.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   221.05 MiB, ( 6342.73 / 21845.34)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "llm_load_tensors:      Metal buffer size =   221.04 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Max\n",
      "ggml_metal_init: picking default device: Apple M1 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   472.75 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    15.25 MiB, ( 6358.73 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    15.25 MiB\n",
      "llama_new_context_with_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   283.64 MiB, ( 6642.38 / 21845.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '7', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: None\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q8_0.gguf',\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad8418e-5456-40d0-9d34-1c4fd914a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "Use pytorch device_name: mps\n",
      "Use pytorch device_name: mps\n"
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eca168c-a2d6-47fa-b965-d61344e5350d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# initialize connection to pinecone\n",
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "index_name = \"os-helper\"\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# connect to the index\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4eb108c-6c98-449b-abb3-e4627e593e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'as_chat_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpinecone_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_chat_engine\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'as_chat_engine'"
     ]
    }
   ],
   "source": [
    "pinecone_index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4213ef2e-7b19-4f9e-8479-33194aab7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b42589ec-7f79-4de6-8f2e-9f4674825b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.vectorstores as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3163f322-7c73-44e9-8275-44693136348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_interface = vs.Pinecone.from_existing_index(\n",
    "    index_name, \n",
    "    embedding=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d2ae193-231f-45d5-b89b-8e2a11442ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PineconeVectorStore' object has no attribute 'as_retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PineconeVectorStore' object has no attribute 'as_retriever'"
     ]
    }
   ],
   "source": [
    "vector_store.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82203d3a-b860-40f2-8e6b-9599d6c101c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "016f296a-94b6-4ef8-923a-82bd51b00e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/w6fqcv917fb6nskgcyj7x_800000gn/T/ipykernel_13995/2085093085.py:8: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "# setup our storage (vector db)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "# setup our service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=64,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc045f-da38-4164-96c2-322b73f0db9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a39e4b7-2561-4529-9fe9-8f60a437828f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad65f117b214131833f2b166e853ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/952 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "257d15a4-72e7-4187-86af-7e1048333efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be47a39c-3d14-440f-8a7f-0da220c933e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19857.91 ms\n",
      "llama_print_timings:      sample time =       8.64 ms /    95 runs   (    0.09 ms per token, 10996.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4324.92 ms /   348 tokens (   12.43 ms per token,    80.46 tokens per second)\n",
      "llama_print_timings:        eval time =    8084.99 ms /    94 runs   (   86.01 ms per token,    11.63 tokens per second)\n",
      "llama_print_timings:       total time =   12548.01 ms /   442 tokens\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"How is page fault handled in exokernel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56975205-c67c-46f1-aca3-6c03acded936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In an Exokernel, page faults are handled by the kernel asking an application to give up a page. The application decides which page to give up and communicates this decision to the kernel through an upcall (kernel â†’ app). The kernel then releases the page and updates the TLB entries. If the kernel needs to force a page fault, it can do so by sending a signal to the application or by modifying the memory management policies of the application.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a60a158-517d-497c-9d19-dfe78d2c3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index1 = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f426d2b-fbd1-4f93-b789-048de60a9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store1 = PineconeVectorStore(pinecone_index=pinecone_index1)\n",
    "\n",
    "# Create a StorageContext\n",
    "storage_context1 = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create a VectorStoreIndex\n",
    "index1 = VectorStoreIndex([], storage_context=storage_context1, service_context=service_context)\n",
    "query_engine = index1.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca47c2e4-b539-4e2a-836a-29d07e813a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   19857.91 ms\n",
      "llama_print_timings:      sample time =      20.63 ms /   225 runs   (    0.09 ms per token, 10907.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3748.66 ms /   291 tokens (   12.88 ms per token,    77.63 tokens per second)\n",
      "llama_print_timings:        eval time =   19183.35 ms /   224 runs   (   85.64 ms per token,    11.68 tokens per second)\n",
      "llama_print_timings:       total time =   23275.76 ms /   515 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The authors of the FlexSC paper calculated the direct and indirect costs in the following way:\n",
      "\n",
      "1. Direct cost: The direct cost refers to the mode-switching overhead that occurs when a system call is made from one thread to another. To calculate this, the authors measured the time it took for a thread to switch modes (i.e., from user space to kernel space and vice versa) using a hardware timer. They then used this measurement to estimate the cost of making a system call from one thread to another.\n",
      "2. Indirect cost: The indirect cost refers to the overhead associated with the processor structure pollution that occurs when a system call is made from one thread to another. To calculate this, the authors measured the time it took for the processor to switch from one thread to another using a hardware timer. They then used this measurement to estimate the cost of making a system call from one thread to another.\n",
      "\n",
      "The authors also mentioned that they used a combination of these two costs to calculate the total cost of making a system call from one thread to another in FlexSC.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"In FlexSC paper, how did the authors calculate the direct & indirect cost?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1769525a-c57c-441e-a701-0fa11cf06fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.core.schema.NodeWithScore'>\n",
      "<class 'llama_index.core.schema.NodeWithScore'>\n"
     ]
    }
   ],
   "source": [
    "response.source_nodes[0].node.metadata['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6156d-2ba3-422f-90f4-1a0bdd807926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
